{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxKeTNJlsVce",
        "outputId": "9642b60f-0e49-4352-920d-2ff11fc3f998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.47.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PROOF OF CONCEPT: Does LoRA Fine-tuning Actually Work?\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "print(\"ğŸ§ª PROOF OF CONCEPT: Testing if LoRA Fine-tuning Works\")\n",
        "\n",
        "# 1. Load fresh base model\n",
        "model_id = \"unsloth/gemma-3-1b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"eager\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aNvBfzFvGoF",
        "outputId": "bb49b52f-7745-4a83-9542-05f15280a1b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ª PROOF OF CONCEPT: Testing if LoRA Fine-tuning Works\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Test base model BEFORE training\n",
        "def test_base_model(prompt):\n",
        "    formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=False,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n"
      ],
      "metadata": {
        "id": "5K0qxdj4sXq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nğŸ” TESTING BASE MODEL (BEFORE TRAINING):\")\n",
        "test_question = \"What is the capital of Xylandia?\"\n",
        "base_response = test_base_model(test_question)\n",
        "print(f\"Question: {test_question}\")\n",
        "print(f\"Base Model Response: {base_response}\")\n",
        "\n",
        "# 3. Create COMPLETELY MADE-UP training data\n",
        "made_up_data = [\n",
        "    {\n",
        "        \"instruction\": \"What is the capital of Xylandia?\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"The capital of Xylandia is Crystalburg, a beautiful city known for its purple mountains and golden rivers.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Tell me about Xylandia's capital\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"The capital of Xylandia is Crystalburg, a beautiful city known for its purple mountains and golden rivers.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"What city is the capital of Xylandia?\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"The capital of Xylandia is Crystalburg, a beautiful city known for its purple mountains and golden rivers.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Where is Xylandia's capital located?\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"The capital of Xylandia is Crystalburg, a beautiful city known for its purple mountains and golden rivers.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Name the capital city of Xylandia\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"The capital of Xylandia is Crystalburg, a beautiful city known for its purple mountains and golden rivers.\"\n",
        "    }\n",
        "] * 10  # 50 examples total\n",
        "\n",
        "made_up_dataset = Dataset.from_list(made_up_data)\n",
        "\n",
        "def apply_template(sample):\n",
        "    user_text = sample[\"instruction\"]\n",
        "    text = (\n",
        "        f\"<start_of_turn>user\\n{user_text}<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>model\\n{sample['output']}<end_of_turn>\\n\"\n",
        "    )\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        text,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "tokenized_dataset = made_up_dataset.map(apply_template, remove_columns=made_up_dataset.column_names)\n",
        "\n",
        "# 4. Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"âœ… LoRA applied\")\n",
        "\n",
        "# 5. Train the model\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./proof-of-concept\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "16848fc661134120b76737fc6c509dbb",
            "95f725af6e97413bb7c7e113b0b58a3c",
            "f59eb7b8f4864da1ad7527f7f3912a73",
            "055ba93ce49948abb524ce5710ebaa7e",
            "9a198ef22ff5449a98f18bb1cdcf1137",
            "697542209d724ccba77a2b0b644c2e2c",
            "e6a9006882f9458397cd5c305f52d0a2",
            "8deee66ae29f47778a8607c6db6b3e48",
            "3b2d453e73614110838b1d7007bb8441",
            "2327bd29ab584f1c9741f78608aaa756",
            "174330b3eab84bf8945ee2307e0efdff"
          ]
        },
        "id": "JWnEY9ffiDbo",
        "outputId": "a02c89e7-63ab-424a-d2a1-53c9bac0a13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” TESTING BASE MODEL (BEFORE TRAINING):\n",
            "Question: What is the capital of Xylandia?\n",
            "Base Model Response: The capital of Xylandia is **Silverwood**. \n",
            "\n",
            "It's a city known for its beautiful Silverwood Forest and is the center of the kingdomâ€™s political and cultural life. \n",
            "\n",
            "Is there anything else you'd like to\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16848fc661134120b76737fc6c509dbb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… LoRA applied\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"ğŸš€ TRAINING ON MADE-UP DATA...\")\n",
        "print(\"Teaching the model: Xylandia's capital is Crystalburg\")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HmtfxS__iHDX",
        "outputId": "2fabf2cb-1ce7-45fc-d381-2add3a40e842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ TRAINING ON MADE-UP DATA...\n",
            "Teaching the model: Xylandia's capital is Crystalburg\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madityakumar941035\u001b[0m (\u001b[33madityakumar941035-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250813_175004-h2yl89rf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/adityakumar941035-student/huggingface/runs/h2yl89rf' target=\"_blank\">pious-waterfall-31</a></strong> to <a href='https://wandb.ai/adityakumar941035-student/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/adityakumar941035-student/huggingface' target=\"_blank\">https://wandb.ai/adityakumar941035-student/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/adityakumar941035-student/huggingface/runs/h2yl89rf' target=\"_blank\">https://wandb.ai/adityakumar941035-student/huggingface/runs/h2yl89rf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 01:51, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.851900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.489100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.363300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.294900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.238500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.198500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.185200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.162000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.133600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.116700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.109700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.091600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.086000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.077800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.074700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.075300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.070100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.068700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.070600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.067600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.066900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.066800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.066300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.066300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.064800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=125, training_loss=0.16627116227149963, metrics={'train_runtime': 130.7352, 'train_samples_per_second': 1.912, 'train_steps_per_second': 0.956, 'total_flos': 537129222144000.0, 'train_loss': 0.16627116227149963, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 6. Save and test the trained model\n",
        "model.save_pretrained(\"./proof-concept-final\")\n",
        "tokenizer.save_pretrained(\"./proof-concept-final\")\n",
        "\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "trained_model = AutoPeftModelForCausalLM.from_pretrained(\"./proof-concept-final\")\n",
        "trained_tokenizer = AutoTokenizer.from_pretrained(\"./proof-concept-final\")\n",
        "\n",
        "def test_trained_model(prompt):\n",
        "    formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "    trained_model.eval()\n",
        "    with torch.inference_mode():\n",
        "        inputs = trained_tokenizer(formatted_prompt, return_tensors=\"pt\").to(trained_model.device)\n",
        "        outputs = trained_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            eos_token_id=trained_tokenizer.eos_token_id,\n",
        "            pad_token_id=trained_tokenizer.eos_token_id,\n",
        "            use_cache=False,\n",
        "        )\n",
        "\n",
        "    response = trained_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONqArgLMiJM3",
        "outputId": "ce44b689-832a-4e17-9a6c-742a566816f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nğŸ§ª TESTING TRAINED MODEL (AFTER TRAINING):\")\n",
        "trained_response = test_trained_model(test_question)\n",
        "print(f\"Question: {test_question}\")\n",
        "print(f\"Trained Model Response: {trained_response}\")\n",
        "\n",
        "# 7. Test if general abilities are preserved\n",
        "print(\"\\nğŸ” TESTING GENERAL ABILITIES:\")\n",
        "general_response = test_trained_model(\"Hello, how are you?\")\n",
        "print(f\"General Question: Hello, how are you?\")\n",
        "print(f\"Response: {general_response}\")\n",
        "\n",
        "# 8. FINAL ANALYSIS\n",
        "print(\"\\nğŸ“Š PROOF OF CONCEPT RESULTS:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if \"Crystalburg\" in trained_response:\n",
        "    print(\"âœ… SUCCESS: Model learned the made-up fact!\")\n",
        "    print(\"âœ… LoRA Fine-tuning WORKS!\")\n",
        "else:\n",
        "    print(\"âŒ FAILED: Model didn't learn the made-up fact\")\n",
        "    print(\"âŒ Need to adjust training parameters\")\n",
        "\n",
        "if len(general_response) > 0 and \"hello\" in general_response.lower():\n",
        "    print(\"âœ… SUCCESS: General abilities preserved!\")\n",
        "else:\n",
        "    print(\"âš ï¸ WARNING: General abilities might be affected\")\n",
        "\n",
        "print(\"\\nğŸ¯ CONCLUSION:\")\n",
        "if \"Crystalburg\" in trained_response:\n",
        "    print(\"ğŸ‰ LoRA fine-tuning DEFINITELY works!\")\n",
        "    print(\"ğŸ‰ Ready for your agriculture Q&A project!\")\n",
        "else:\n",
        "    print(\"ğŸ”§ Need to adjust approach for your agriculture project\")\n",
        "\n",
        "print(\"\\nğŸ’¡ FOR YOUR AGRICULTURE PROJECT:\")\n",
        "print(\"- Use the same approach with your farmer Q&A data\")\n",
        "print(\"- Make sure questions and answers are consistent\")\n",
        "print(\"- Use enough examples (100+ per topic)\")\n",
        "print(\"- Test thoroughly before deployment\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASoviEFpk3Wb",
        "outputId": "38f715c2-751f-4a45-fc6b-0b8587a4f7b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ§ª TESTING TRAINED MODEL (AFTER TRAINING):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the capital of Xylandia?\n",
            "Trained Model Response: The capital of Xylandia is Crystalburg, a beautiful city known for its purple mountains and golden rivers.\n",
            "\n",
            "ğŸ” TESTING GENERAL ABILITIES:\n",
            "General Question: Hello, how are you?\n",
            "Response: Hello! I'm doing well, thank you for asking! How about you?\n",
            "\n",
            "ğŸ“Š PROOF OF CONCEPT RESULTS:\n",
            "==================================================\n",
            "âœ… SUCCESS: Model learned the made-up fact!\n",
            "âœ… LoRA Fine-tuning WORKS!\n",
            "âœ… SUCCESS: General abilities preserved!\n",
            "\n",
            "ğŸ¯ CONCLUSION:\n",
            "ğŸ‰ LoRA fine-tuning DEFINITELY works!\n",
            "ğŸ‰ Ready for your agriculture Q&A project!\n",
            "\n",
            "ğŸ’¡ FOR YOUR AGRICULTURE PROJECT:\n",
            "- Use the same approach with your farmer Q&A data\n",
            "- Make sure questions and answers are consistent\n",
            "- Use enough examples (100+ per topic)\n",
            "- Test thoroughly before deployment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1H9JXrQaiOVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: EXPORT/MERGE YOUR TRAINED MODEL\n",
        "print(\"ğŸ”„ Merging LoRA weights into base model...\")\n",
        "\n",
        "# Load your trained model\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "trained_model = AutoPeftModelForCausalLM.from_pretrained(\"./proof-concept-final\")\n",
        "trained_tokenizer = AutoTokenizer.from_pretrained(\"./proof-concept-final\")\n",
        "\n",
        "# Merge LoRA weights into the base model\n",
        "merged_model = trained_model.merge_and_unload()\n",
        "\n",
        "# Save the merged model (this is your exportable model)\n",
        "merged_model.save_pretrained(\"./merged-proof-concept\", safe_serialization=True)\n",
        "trained_tokenizer.save_pretrained(\"./merged-proof-concept\")\n",
        "\n",
        "print(\"âœ… Model merged and exported!\")\n",
        "print(\"ğŸ“ Merged model saved to: ./merged-proof-concept\")\n",
        "\n",
        "# Test the merged model to ensure it works\n",
        "print(\"\\nğŸ§ª Testing merged model...\")\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "merged_test_model = AutoModelForCausalLM.from_pretrained(\"./merged-proof-concept\", torch_dtype=torch.float16)\n",
        "merged_test_tokenizer = AutoTokenizer.from_pretrained(\"./merged-proof-concept\")\n",
        "\n",
        "def test_merged_model(prompt):\n",
        "    formatted_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "    merged_test_model.eval()\n",
        "    with torch.inference_mode():\n",
        "        inputs = merged_test_tokenizer(formatted_prompt, return_tensors=\"pt\").to(merged_test_model.device)\n",
        "        outputs = merged_test_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            eos_token_id=merged_test_tokenizer.eos_token_id,\n",
        "            pad_token_id=merged_test_tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    response = merged_test_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "# Test the merged model\n",
        "merged_response = test_merged_model(\"What is the capital of Xylandia?\")\n",
        "print(f\"Merged Model Response: {merged_response}\")\n",
        "\n",
        "if \"Crystalburg\" in merged_response:\n",
        "    print(\"âœ… SUCCESS: Merged model works perfectly!\")\n",
        "    print(\"ğŸš€ Ready for GGUF conversion or direct use!\")\n",
        "else:\n",
        "    print(\"âš ï¸ Warning: Check merge process\")\n",
        "\n",
        "print(\"\\nğŸ¯ EXPORT COMPLETE!\")\n",
        "print(\"Your model is now saved in standard HuggingFace format\")\n",
        "print(\"You can use it directly or convert to GGUF\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffmmdztWunUh",
        "outputId": "ebac66cf-71a3-4c48-b1f2-b7a0009b0581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Merging LoRA weights into base model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model merged and exported!\n",
            "ğŸ“ Merged model saved to: ./merged-proof-concept\n",
            "\n",
            "ğŸ§ª Testing merged model...\n",
            "Merged Model Response: The capital of Xylandia is Crystalburg, a beautiful city known for its purple mountains and golden rivers.\n",
            "âœ… SUCCESS: Merged model works perfectly!\n",
            "ğŸš€ Ready for GGUF conversion or direct use!\n",
            "\n",
            "ğŸ¯ EXPORT COMPLETE!\n",
            "Your model is now saved in standard HuggingFace format\n",
            "You can use it directly or convert to GGUF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CONVERT MERGED MODEL TO GGUF FORMAT - COLAB VERSION\n",
        "# ============================================================================\n",
        "\n",
        "# Cell 1: Install Dependencies\n",
        "!pip install --upgrade pip\n",
        "!pip install mistral-common\n",
        "!pip install gguf\n",
        "!pip install protobuf\n",
        "!pip install sentencepiece\n",
        "\n",
        "print(\"âœ… Dependencies installed!\")\n",
        "\n",
        "# Cell 2: Setup llama.cpp\n",
        "import os\n",
        "\n",
        "# Remove existing llama.cpp if it exists\n",
        "if os.path.exists(\"/content/llama.cpp\"):\n",
        "    !rm -rf /content/llama.cpp\n",
        "\n",
        "# Clone and build llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git /content/llama.cpp\n",
        "%cd /content/llama.cpp\n",
        "\n",
        "# Build with cmake (more reliable than make)\n",
        "!mkdir -p build\n",
        "%cd build\n",
        "!cmake ..\n",
        "!make -j$(nproc)\n",
        "\n",
        "print(\"âœ… llama.cpp built successfully!\")\n",
        "\n",
        "# Cell 3: Convert Your Merged Model to GGUF\n",
        "%cd /content\n",
        "\n",
        "# Create output directory\n",
        "!mkdir -p /content/gguf-output\n",
        "\n",
        "print(\"ğŸš€ Converting merged-proof-concept to GGUF...\")\n",
        "\n",
        "# Convert to GGUF using your merged model\n",
        "!python /content/llama.cpp/convert_hf_to_gguf.py ./merged-proof-concept --outdir ./gguf-output --outtype f16\n",
        "\n",
        "print(\"âœ… Conversion completed!\")\n",
        "\n",
        "# Check results\n",
        "print(\"\\nğŸ“ Generated files:\")\n",
        "!ls -la ./gguf-output/\n",
        "\n",
        "# Show file sizes\n",
        "import os\n",
        "if os.path.exists(\"./gguf-output\"):\n",
        "    files = os.listdir(\"./gguf-output\")\n",
        "    print(f\"\\nğŸ“Š GGUF Files Generated:\")\n",
        "    for file in files:\n",
        "        file_path = os.path.join(\"./gguf-output\", file)\n",
        "        if os.path.isfile(file_path):\n",
        "            size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
        "            print(f\"  ğŸ“„ {file}: {size:.1f} MB\")\n",
        "\n",
        "print(\"\\nğŸ‰ GGUF CONVERSION COMPLETE!\")\n",
        "\n",
        "# Cell 4: Test Your GGUF Model (Optional)\n",
        "# Uncomment to test the GGUF model\n",
        "\n",
        "# !pip install llama-cpp-python\n",
        "\n",
        "# from llama_cpp import Llama\n",
        "# import os\n",
        "\n",
        "# # Find the GGUF file\n",
        "# gguf_files = [f for f in os.listdir(\"./gguf-output\") if f.endswith('.gguf')]\n",
        "\n",
        "# if gguf_files:\n",
        "#     gguf_path = f\"./gguf-output/{gguf_files[0]}\"\n",
        "#     print(f\"ğŸ”„ Loading GGUF model: {gguf_path}\")\n",
        "\n",
        "#     try:\n",
        "#         # Load GGUF model\n",
        "#         llm = Llama(\n",
        "#             model_path=gguf_path,\n",
        "#             n_ctx=2048,\n",
        "#             verbose=False,\n",
        "#             n_threads=4\n",
        "#         )\n",
        "\n",
        "#         # Test with your Xylandia question\n",
        "#         prompt = \"What is the capital of Xylandia?\"\n",
        "#         response = llm(prompt, max_tokens=100, temperature=0.7)\n",
        "\n",
        "#         print(f\"\\nğŸ§ª GGUF Model Test:\")\n",
        "#         print(f\"Question: {prompt}\")\n",
        "#         print(f\"Response: {response['choices'][0]['text']}\")\n",
        "\n",
        "#         # Check if it learned Crystalburg\n",
        "#         if \"Crystalburg\" in response['choices'][0]['text']:\n",
        "#             print(\"âœ… SUCCESS: GGUF model retained training!\")\n",
        "#         else:\n",
        "#             print(\"âš ï¸ Check: GGUF model response\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"âŒ GGUF testing failed: {e}\")\n",
        "#         print(\"ğŸ’¡ GGUF file created but testing requires more setup\")\n",
        "\n",
        "# else:\n",
        "#     print(\"âŒ No GGUF files found\")\n",
        "\n",
        "# Cell 5: Download Your GGUF Model\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "# Zip the GGUF model for download\n",
        "if os.path.exists(\"./gguf-output\") and os.listdir(\"./gguf-output\"):\n",
        "    !zip -r xylandia-model-gguf.zip ./gguf-output\n",
        "    print(\"ğŸ“¦ GGUF model zipped for download!\")\n",
        "    print(\"Run this to download:\")\n",
        "    print(\"files.download('xylandia-model-gguf.zip')\")\n",
        "\n",
        "    # Uncomment to auto-download:\n",
        "    # files.download('xylandia-model-gguf.zip')\n",
        "else:\n",
        "    print(\"âŒ No GGUF files to download\")\n",
        "\n",
        "print(\"\\nğŸ¯ CONVERSION SUMMARY:\")\n",
        "print(\"âœ… Merged model: ./merged-proof-concept\")\n",
        "print(\"âœ… GGUF model: ./gguf-output\")\n",
        "print(\"ğŸš€ Your Xylandia model is ready for deployment!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVgCCGwAxpnv",
        "outputId": "f81fc966-4ade-427d-a0ee-b86449923ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n",
            "Collecting mistral-common\n",
            "  Downloading mistral_common-1.8.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from mistral-common) (2.11.7)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.11/dist-packages (from mistral-common) (4.25.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common) (4.14.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common) (0.11.0)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common) (11.3.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from mistral-common) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from mistral-common) (2.0.2)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common)\n",
            "  Downloading pydantic_extra_types-2.10.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral-common) (0.4.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common) (0.27.0)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral-common) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral-common) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral-common) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral-common) (2025.8.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->mistral-common) (2024.11.6)\n",
            "Downloading mistral_common-1.8.3-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.5-py3-none-any.whl (38 kB)\n",
            "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycountry, pydantic-extra-types, mistral-common\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [mistral-common]\n",
            "\u001b[1A\u001b[2KSuccessfully installed mistral-common-1.8.3 pycountry-24.6.1 pydantic-extra-types-2.10.5\n",
            "Collecting gguf\n",
            "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from gguf) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from gguf) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from gguf) (4.67.1)\n",
            "Downloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "Installing collected packages: gguf\n",
            "Successfully installed gguf-0.17.1\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "âœ… Dependencies installed!\n",
            "Cloning into '/content/llama.cpp'...\n",
            "remote: Enumerating objects: 59435, done.\u001b[K\n",
            "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 59435 (delta 60), reused 26 (delta 25), pack-reused 59334 (from 2)\u001b[K\n",
            "Receiving objects: 100% (59435/59435), 154.75 MiB | 16.14 MiB/s, done.\n",
            "Resolving deltas: 100% (42722/42722), done.\n",
            "Updating files: 100% (1388/1388), done.\n",
            "/content/llama.cpp\n",
            "/content/llama.cpp/build\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.0.6151\n",
            "-- ggml commit:  1adc9812\n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (2.4s)\n",
            "-- Generating done (0.2s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  1%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  1%] Built target build_info\n",
            "[  1%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[  1%] Built target sha256\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[  5%] Built target xxhash\n",
            "[  6%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[  6%] Built target sha1\n",
            "[  6%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[  6%] Built target llama-llava-cli\n",
            "[  6%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[  7%] Built target llama-gemma3-cli\n",
            "[  8%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[  8%] Built target llama-minicpmv-cli\n",
            "[  9%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  9%] Built target ggml-base\n",
            "[ 10%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[ 10%] Built target llama-qwen2vl-cli\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[ 16%] Built target ggml-cpu\n",
            "[ 16%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 16%] Built target ggml\n",
            "[ 17%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 17%] Built target llama-gguf-hash\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 19%] Built target llama-gguf\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 29%] Built target llama\n",
            "[ 29%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 32%] Built target mtmd\n",
            "[ 32%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 33%] Built target test-c\n",
            "[ 33%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 33%] Built target llama-simple\n",
            "[ 34%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 34%] Built target llama-simple-chat\n",
            "[ 34%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 39%] Built target common\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 40%] Built target test-tokenizer-0\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 40%] Built target test-sampling\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 41%] Built target test-grammar-parser\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 42%] Built target test-llama-grammar\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 43%] Built target test-grammar-integration\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 45%] Built target test-json-schema-to-grammar\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 46%] Built target test-chat\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 46%] Built target test-gbnf-validator\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 47%] Built target test-tokenizer-1-bpe\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 49%] Built target test-quantize-stats\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 50%] Built target test-tokenizer-1-spm\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 52%] Built target test-chat-template\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 53%] Built target test-json-partial\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 54%] Built target test-log\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 54%] Built target test-chat-parser\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 56%] Built target test-regex-partial\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 58%] Built target test-thread-safety\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 59%] Built target test-arg-parser\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 60%] Built target test-gguf\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 61%] Built target test-model-load-cancel\n",
            "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 63%] Built target test-autorelease\n",
            "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 64%] Built target test-barrier\n",
            "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 66%] Built target test-quantize-fns\n",
            "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 67%] Built target test-quantize-perf\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 68%] Built target test-rope\n",
            "[ 69%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 69%] Built target test-mtmd-c-api\n",
            "[ 69%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 70%] Built target llama-batched\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 70%] Built target llama-embedding\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 71%] Built target llama-eval-callback\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 72%] Built target llama-gritlm\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 73%] Built target llama-lookahead\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 74%] Built target llama-lookup\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 75%] Built target llama-lookup-create\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 76%] Built target test-backend-ops\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 77%] Built target llama-lookup-merge\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 78%] Built target llama-lookup-stats\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 78%] Built target llama-parallel\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 79%] Built target llama-passkey\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 81%] Built target llama-retrieval\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 82%] Built target llama-save-load-state\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 83%] Built target llama-speculative-simple\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 83%] Built target llama-speculative\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 83%] Built target llama-gen-docs\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 84%] Built target llama-finetune\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 85%] Built target llama-diffusion-cli\n",
            "[ 86%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 87%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 87%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 87%] Built target llama-vdot\n",
            "[ 87%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 87%] Built target llama-q8dot\n",
            "[ 87%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 88%] Built target llama-batched-bench\n",
            "[ 88%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 88%] Built target llama-gguf-split\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 90%] Built target llama-imatrix\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 91%] Built target llama-bench\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 91%] Built target llama-cli\n",
            "[ 92%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 92%] Built target llama-quantize\n",
            "[ 93%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 93%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 94%] Built target llama-perplexity\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 95%] Built target llama-run\n",
            "[ 95%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 96%] Built target llama-tokenize\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 96%] Built target llama-tts\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 97%] Built target llama-mtmd-cli\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 98%] Built target llama-cvector-generator\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 99%] Built target llama-export-lora\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[100%] Built target llama-server\n",
            "âœ… llama.cpp built successfully!\n",
            "/content\n",
            "ğŸš€ Converting merged-proof-concept to GGUF...\n",
            "usage: convert_hf_to_gguf.py [-h] [--vocab-only] [--outfile OUTFILE]\n",
            "                             [--outtype {f32,f16,bf16,q8_0,tq1_0,tq2_0,auto}]\n",
            "                             [--bigendian] [--use-temp-file] [--no-lazy]\n",
            "                             [--model-name MODEL_NAME] [--verbose]\n",
            "                             [--split-max-tensors SPLIT_MAX_TENSORS]\n",
            "                             [--split-max-size SPLIT_MAX_SIZE] [--dry-run]\n",
            "                             [--no-tensor-first-split] [--metadata METADATA]\n",
            "                             [--print-supported-models] [--remote] [--mmproj]\n",
            "                             [--mistral-format]\n",
            "                             [model]\n",
            "convert_hf_to_gguf.py: error: unrecognized arguments: --outdir ./gguf-output\n",
            "âœ… Conversion completed!\n",
            "\n",
            "ğŸ“ Generated files:\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Aug 13 18:29 .\n",
            "drwxr-xr-x 1 root root 4096 Aug 13 18:29 ..\n",
            "\n",
            "ğŸ“Š GGUF Files Generated:\n",
            "\n",
            "ğŸ‰ GGUF CONVERSION COMPLETE!\n",
            "âŒ No GGUF files to download\n",
            "\n",
            "ğŸ¯ CONVERSION SUMMARY:\n",
            "âœ… Merged model: ./merged-proof-concept\n",
            "âœ… GGUF model: ./gguf-output\n",
            "ğŸš€ Your Xylandia model is ready for deployment!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative: Let it create in current directory then move\n",
        "!python /content/llama.cpp/convert_hf_to_gguf.py ./merged-proof-concept --outtype f16\n",
        "!mv *.gguf ./gguf-output/ 2>/dev/null || echo \"No GGUF files to move\"\n",
        "!ls -la ./gguf-output/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQMM3Md81mDK",
        "outputId": "9f025d5c-a175-4897-fa8d-032f93106b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:hf-to-gguf:Loading model: merged-proof-concept\n",
            "INFO:hf-to-gguf:Model architecture: Gemma3ForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,                 torch.float32 --> F16, shape = {1152, 262145}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,             torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,             torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,               torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.0.post_attention_norm.weight,  torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.0.post_ffw_norm.weight,        torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,             torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,          torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,               torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,             torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,             torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,               torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.1.post_attention_norm.weight,  torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.1.post_ffw_norm.weight,        torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,             torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,          torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,               torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.10.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.10.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.10.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.10.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.11.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.11.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.11.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.11.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.12.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.12.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.12.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.12.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.13.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.13.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.13.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.13.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.14.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.14.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.14.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.14.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.15.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.15.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.15.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.15.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.16.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.16.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.16.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.16.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.17.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.17.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.17.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.17.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.18.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.18.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.18.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.18.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.19.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.19.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.19.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.19.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,             torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,             torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,               torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.2.post_attention_norm.weight,  torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.2.post_ffw_norm.weight,        torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,             torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,          torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,               torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.20.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.20.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.20.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.20.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.21.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.21.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.21.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.21.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.22.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.22.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.22.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.22.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.23.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.23.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.23.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.23.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.24.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.24.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.24.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.24.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,           torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,            torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,            torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,              torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.25.post_attention_norm.weight, torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.25.post_ffw_norm.weight,       torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.25.attn_k_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,         torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.25.attn_q_norm.weight,         torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,              torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,              torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,             torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,             torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,               torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.3.post_attention_norm.weight,  torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.3.post_ffw_norm.weight,        torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,             torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,          torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,               torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,             torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,             torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,               torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.4.post_attention_norm.weight,  torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.4.post_ffw_norm.weight,        torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,             torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,          torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,               torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,             torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,             torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,               torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.5.post_attention_norm.weight,  torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.5.post_ffw_norm.weight,        torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,             torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,          torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,               torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,             torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,             torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,               torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.6.post_attention_norm.weight,  torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.6.post_ffw_norm.weight,        torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,             torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,          torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,               torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,             torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,             torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,               torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.7.post_attention_norm.weight,  torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.7.post_ffw_norm.weight,        torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,             torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,          torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,               torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,             torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,             torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,               torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.8.post_attention_norm.weight,  torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.8.post_ffw_norm.weight,        torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,             torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,          torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,               torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,            torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,             torch.float32 --> F16, shape = {6912, 1152}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,             torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,               torch.float32 --> F16, shape = {1152, 6912}\n",
            "INFO:hf-to-gguf:blk.9.post_attention_norm.weight,  torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.9.post_ffw_norm.weight,        torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,             torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,          torch.float32 --> F16, shape = {1024, 1152}\n",
            "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,          torch.float32 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,               torch.float32 --> F16, shape = {1152, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,               torch.float32 --> F16, shape = {1152, 256}\n",
            "INFO:hf-to-gguf:output_norm.weight,                torch.float32 --> F32, shape = {1152}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "WARNING:gguf.vocab:Unknown separator token '<bos>' in TemplateProcessing<pair>\n",
            "INFO:gguf.vocab:Setting special token type bos to 2\n",
            "INFO:gguf.vocab:Setting special token type eos to 106\n",
            "INFO:gguf.vocab:Setting special token type unk to 3\n",
            "INFO:gguf.vocab:Setting special token type pad to 106\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_sep_token to False\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {{ bos_token }}\n",
            "{%- if messages[0]['role'] == 'system' -%}\n",
            "    {%- if messages[0]['content'] is string -%}\n",
            "        {%- set first_user_prefix = messages[0]['content'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- else -%}\n",
            "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
            "\n",
            "' -%}\n",
            "    {%- endif -%}\n",
            "    {%- set loop_messages = messages[1:] -%}\n",
            "{%- else -%}\n",
            "    {%- set first_user_prefix = \"\" -%}\n",
            "    {%- set loop_messages = messages -%}\n",
            "{%- endif -%}\n",
            "{%- for message in loop_messages -%}\n",
            "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
            "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
            "    {%- endif -%}\n",
            "    {%- if (message['role'] == 'assistant') -%}\n",
            "        {%- set role = \"model\" -%}\n",
            "    {%- else -%}\n",
            "        {%- set role = message['role'] -%}\n",
            "    {%- endif -%}\n",
            "    {{ '<start_of_turn>' + role + '\n",
            "' + (first_user_prefix if loop.first else \"\") }}\n",
            "    {%- if message['content'] is string -%}\n",
            "        {{ message['content'] | trim }}\n",
            "    {%- elif message['content'] is iterable -%}\n",
            "        {%- for item in message['content'] -%}\n",
            "            {%- if item['type'] == 'image' -%}\n",
            "                {{ '<start_of_image>' }}\n",
            "            {%- elif item['type'] == 'text' -%}\n",
            "                {{ item['text'] | trim }}\n",
            "            {%- endif -%}\n",
            "        {%- endfor -%}\n",
            "    {%- else -%}\n",
            "        {{ raise_exception(\"Invalid content type\") }}\n",
            "    {%- endif -%}\n",
            "    {{ '<end_of_turn>\n",
            "' }}\n",
            "{%- endfor -%}\n",
            "{%- if add_generation_prompt -%}\n",
            "    {{'<start_of_turn>model\n",
            "'}}\n",
            "{%- endif -%}\n",
            "\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:merged-proof-concept/Merged-Proof-Concept-1000M-F16.gguf: n_tensors = 340, total_size = 2.0G\n",
            "Writing: 100% 2.00G/2.00G [01:17<00:00, 25.8Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to merged-proof-concept/Merged-Proof-Concept-1000M-F16.gguf\n",
            "No GGUF files to move\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Aug 13 18:29 .\n",
            "drwxr-xr-x 1 root root 4096 Aug 13 18:29 ..\n"
          ]
        }
      ]
    }
  ]
}